<!DOCTYPE html>
<!--[if IE]><![endif]-->
<html>
  
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>Data import and export jobs overview | WIKA Documentation </title>
    <meta name="viewport" content="width=device-width">
    <meta name="title" content="Data import and export jobs overview | WIKA Documentation ">
    <meta name="generator" content="docfx 2.56.6.0">
    
    <link rel="shortcut icon" href="../../../../microsoft-dynamics-crm-365-icon.ico">
    <link rel="stylesheet" href="../../../../styles/docfx.vendor.css">
    <link rel="stylesheet" href="../../../../styles/docfx.css">
    <link rel="stylesheet" href="../../../../styles/main.css">
    <link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet"> 
    <meta property="docfx:navrel" content="../../../../toc.html">
    <meta property="docfx:tocrel" content="../toc.html">
    
    
    
  </head>  <body data-spy="scroll" data-target="#affix" data-offset="120">
    <div id="wrapper">
      <header>
        
        <nav id="autocollapse" class="navbar navbar-inverse ng-scope" role="navigation">
          <div class="container">
            <div class="navbar-header">
              <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navbar">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
              </button>
              
              <a class="navbar-brand" href="../../../../index.html">
                <img id="logo" class="svg" src="../../../../logo.svg" alt="">
              </a>
            </div>
            <div class="collapse navbar-collapse" id="navbar">
              <form class="navbar-form navbar-right" role="search" id="search">
                <div class="form-group">
                  <input type="text" class="form-control" id="search-query" placeholder="Search" autocomplete="off">
                </div>
              </form>
            </div>
          </div>
        </nav>
        
        <div class="subnav navbar navbar-default">
          <div class="container hide-when-search" id="breadcrumb">
            <ul class="breadcrumb">
              <li></li>
            </ul>
          </div>
        </div>
      </header>
      <div role="main" class="container body-content hide-when-search">
        
        <div class="sidenav hide-when-search">
          <a class="btn toc-toggle collapse" data-toggle="collapse" href="#sidetoggle" aria-expanded="false" aria-controls="sidetoggle">Show / Hide Table of Contents</a>
          <div class="sidetoggle collapse" id="sidetoggle">
            <div id="sidetoc"></div>
          </div>
        </div>
        <div class="article row grid-right">
          <div class="col-md-10">
            <article class="content wrap" id="_content" data-uid="">
<h1 id="data-import-and-export-jobs-overview">Data import and export jobs overview</h1>


<p>To create and manage data import and export jobs, you use the <strong>Data management</strong> workspace. By default, the data import and export process creates a staging table for each entity in the target database. Staging tables let you verify, clean up, or convert data before you move it.</p>
<div class="NOTE">
<h5>Note</h5>
<p>This topic assumes that you are familiar with <a href="https://docs.wika.com/en-us/dynamics365/supply-chain/fin-ops-core/dev-itpro/data-entities/data-entities">data entities (This is an external linThis link was changed due to HTMLfromRepoGenerator)</a>.</p>
</div>
<h2 id="data-importexport-process">Data import/export process</h2>
<p>Here are the steps to import or export data.</p>
<ol>
<li><p>Create an import or export job where you complete the following tasks:</p>
<ul>
<li>Define the project category.</li>
<li>Identify the entities to import or export.</li>
<li>Set the data format for the job.</li>
<li>Sequence the entities, so that they are processed in logical groups and in an order that makes sense.</li>
<li>Determine whether to use staging tables.</li>
</ul>
</li>
<li><p>Validate that the source data and target data are mapped correctly.</p>
</li>
<li><p>Verify the security for your import or export job.</p>
</li>
<li><p>Run the import or export job.</p>
</li>
<li><p>Validate that the job ran as expected by reviewing the job history.</p>
</li>
<li><p>Clean up the staging tables.</p>
</li>
</ol>
<p>The remaining sections of this topic provide more details about each step of the process.</p>
<div class="NOTE">
<h5>Note</h5>
<p>In order to refresh the Data import/export form to see the latest progress, use the form refresh icon. Browser level refresh is not recommended because it will interrupt any import/export jobs that are not run in batch.</p>
</div>
<h2 id="create-an-import-or-export-job">Create an import or export job</h2>
<p>A data import or export job can be run one time or many times.</p>
<h3 id="define-the-project-category">Define the project category</h3>
<p>We recommend that you take the time to select an appropriate project category for your import or export job. Project categories can help you manage related jobs.</p>
<h3 id="identify-the-entities-to-import-or-export">Identify the entities to import or export</h3>
<p>You can add specific entities to an import or export job or select a template to apply. Templates fill a job with a list of entities. The <strong>Apply template</strong> option is available after you give the job a name and save the job.</p>
<h3 id="set-the-data-format-for-the-job">Set the data format for the job</h3>
<p>When you select an entity, you must select the format of the data that will be exported or imported. You define formats by using the <strong>Data sources setup</strong> tile. A source data format is a combination of <strong>Type</strong>, <strong>File format</strong>, <strong>Row delimiter</strong> and <strong>Column delimiter</strong>. There are also other attributes, but these are the key ones to understand. The following table lists the valid combinations.</p>
<table>
<thead>
<tr>
<th>File Format</th>
<th>Row/Column delimiter</th>
<th>XML Style</th>
</tr>
</thead>
<tbody>
<tr>
<td>Excel</td>
<td>Excel</td>
<td>-NA-</td>
</tr>
<tr>
<td>XML</td>
<td>-NA-</td>
<td>XML-Element XML-Attribute</td>
</tr>
<tr>
<td>Delimited, fixed width</td>
<td>Comma, semicolon, tab, vertical bar, colon</td>
<td>-NA-</td>
</tr>
</tbody>
</table>
<h3 id="sequence-the-entities">Sequence the entities</h3>
<p>Entities can be sequenced in a data template, or in import and export jobs. When you run a job that contains more than one data entity, you must make sure that the data entities are correctly sequenced. You sequence entities primarily so that you can address any functional dependencies among entities. If entities don’t have any functional dependencies, they can be scheduled for parallel import or export.</p>
<h4 id="execution-units-levels-and-sequences">Execution units, levels, and sequences</h4>
<p>The execution unit, level in the execution unit, and sequence of an entity help control the order that the data is exported or imported in.</p>
<ul>
<li>Entities in different execution units are processed in parallel.</li>
<li>In each execution unit, entities are processed in parallel if they have the same level.</li>
<li>In each level, entities are processed according to their sequence number in that level.</li>
<li>After one level has been processed, the next level is processed.</li>
</ul>
<h4 id="resequencing">Resequencing</h4>
<p>You might want to resequence your entities in the following situations:</p>
<ul>
<li>If only one data job is used for all your changes, you can use resequencing options to optimize the execution time for the full job. In these cases, you can use the execution unit to represent the module, the level to represent the feature area in the module, and the sequence to represent the entity. By using this approach, you can work across modules in parallel, but you can still work in sequence in a module. To help guarantee that parallel operations succeed, you must consider all dependencies.</li>
<li>If multiple data jobs are used (for example, one job for each module), you can use sequencing to affect the level and sequence of entities for optimal execution.</li>
<li>If there are no dependencies at all, you can sequence entities at different execution units for maximum optimization.</li>
</ul>
<p>The <strong>Resequencing</strong> menu is available when multiple entities are selected. You can resequence based on execution unit, level, or sequence options. You can set an increment to resequence the entities that have been selected. The unit, level, and/or sequence number that is selected for each entity is updated by the specified increment.</p>
<h4 id="sorting">Sorting</h4>
<p>Use can use the <strong>Sort by</strong> option to view the entity list in sequential order.</p>
<h3 id="truncating">Truncating</h3>
<p>For import projects, you can choose to truncate records in the entities prior to import. This is useful if your records must be imported into a clean set of tables. This setting is off by default.</p>
<h2 id="validate-that-the-source-data-and-target-data-are-mapped-correctly">Validate that the source data and target data are mapped correctly</h2>
<p>Mapping is a function that applies to both import and export jobs.</p>
<ul>
<li>In the context of an import job, mapping describes which columns in the source file become the columns in the staging table. Therefore, the system can determine which column data in the source file must be copied into which column of the staging table.</li>
<li>In the context of an export job, mapping describes which columns of the staging table (that is, the source) become the columns in the target file.</li>
</ul>
<p>If the column names in the staging table and the file match, the system automatically establishes the mapping, based on the names. However, if the names differ, columns aren’t mapped automatically. In these cases, you must complete the mapping by selecting the <strong>View map</strong> option on the entity in the data job.</p>
<p>There are two mapping views: <strong>Mapping visualization</strong>, which is the default view, and <strong>Mapping details</strong>. A red asterisk (*) identifies any required fields in the entity. These fields must be mapped before you can work with the entity. You can unmap other fields as you require when you work with the entity. To unmap a field, select the field in either the <strong>Entity</strong> column or the <strong>Source</strong> column, and then select <strong>Delete selection</strong>. Select <strong>Save</strong> to save your changes, and then close the page to return to the project. You can use the same process to edit the field mapping from source to staging after you import.</p>
<p>You can generate a mapping on the page by selecting <strong>Generate source mapping</strong>. A generated mapping behaves like an automatic mapping. Therefore, you must manually map any unmapped fields.</p>
<p><img src="./media/dixf-map.png" alt="Data mapping"></p>
<h2 id="verify-the-security-for-your-import-or-export-job">Verify the security for your import or export job</h2>
<p>Access to the <strong>Data management</strong> workspace can be restricted, so that non-administrator users can access only specific data jobs. Access to a data job implies full access to the execution history of that job and access to the staging tables. Therefore, you must make sure that appropriate access controls are in place when you create a data job.</p>
<h3 id="secure-a-job-by-roles-and-users">Secure a job by roles and users</h3>
<p>Use the <strong>Applicable roles</strong> menu to restrict the job to one or more security roles. Only users in those roles will have access to the job.</p>
<p>You can also restrict a job to specific users. When you secure a job by users instead of roles, there is more control if multiple users are assigned to a role.</p>
<h3 id="secure-a-job-by-legal-entity">Secure a job by legal entity</h3>
<p>Data jobs are global in nature. Therefore, if a data job was created and used in a legal entity, the job will be visible in other legal entities in the system. This default behavior might be preferred in some application scenarios. For example, an organization that imports invoices by using data entities might provide a centralized invoice processing team that is responsible for managing invoice errors for all divisions in the organization. In this scenario, it’s useful for the centralized invoice processing team to have access to invoice import jobs from all legal entities. Therefore, the default behavior meets the requirement from a legal entity perspective.</p>
<p>However, an organization might want to have invoice processing teams per legal entity. In this case, a team in a legal entity should have access only to the invoice import job in its own legal entity. To meet this requirement, you can configure legal entity–based access control on the data jobs by using the <strong>Applicable legal entities</strong> menu inside the data job. After the configuration is done, users can see only jobs that are available in the legal entity that they are currently signed in to. To see jobs from another legal entity, users must switch to that legal entity.</p>
<p>A job can be secured by roles, users, and legal entity at the same time.</p>
<h2 id="run-the-import-or-export-job">Run the import or export job</h2>
<p>You can run a job one time by selecting the <strong>Import</strong> or <strong>Export</strong> button after you define the job. To set up a recurring job, select <strong>Create recurring data job</strong>.</p>
<div class="NOTE">
<h5>Note</h5>
<p>An import or an export job can be run by selecting the <strong>Import</strong> or <strong>Export</strong> button. This will schedule a batch job to run only once. The job may not execute immediately if batch service is throttling due to the load on the batch service. The jobs can also be run synchronously by selecting <strong>Import now</strong> or <strong>Export now</strong>. This starts the job immediately and is useful if the batch does not start due to throttling. The jobs can also be scheduled to execute at a later time. This can be done by choosing the <strong>Run in batch</strong> option. Batch resources are subject to throttling, so the batch job might not start immediately. Using a batch is the recommended option because it will also help with large volumes of data that need to be imported or exported. Batch jobs can be scheduled to run on a specific batch group, which allows more control from a load balancing perspective.</p>
</div>
<h2 id="validate-that-the-job-ran-as-expected">Validate that the job ran as expected</h2>
<p>The job history is available for troubleshooting and investigation on both import and export jobs. Historical job runs are organized by time ranges.</p>
<p><img src="./media/dixf-job-history.md.png" alt="Job history ranges"></p>
<p>Each job run provides the following details:</p>
<ul>
<li>Execution details</li>
<li>Execution log</li>
</ul>
<p>Execution details show the state of each data entity that the job processed. Therefore, you can quickly find the following information:</p>
<ul>
<li>Which entities were processed.</li>
<li>For an entity, how many records were successfully processed, and how many failed.</li>
<li>The staging records for each entity.</li>
</ul>
<p>You can download the staging data in a file for export jobs, or you can download it as a package for import and export jobs.</p>
<p>From the execution details, you can also open the execution log.</p>
<h2 id="parallel-imports">Parallel imports</h2>
<p>To speed up the import of data, parallel processing of importing a file can be enabled if the entity supports parallel imports. To configure the parallel import for an entity, the following steps must be followed.</p>
<ol>
<li><p>Go to <strong>System administration &gt; Workspaces &gt; Data management</strong>.</p>
</li>
<li><p>In the <strong>Import / Export</strong> section, select the <strong>Framework parameters</strong> tile to open the <strong>Data import/export framework parameters</strong> page.</p>
</li>
<li><p>On the <strong>Entity settings</strong> tab, select <strong>Configure entity execution parameters</strong> to open the <strong>Entity import execution parameters</strong> page.</p>
</li>
<li><p>Set the following fields to configure parallel import for an entity:</p>
<ul>
<li>In the <strong>Entity</strong> field, select the entity.</li>
<li>In the <strong>Import threshold record count</strong> field, enter the threshold record count for import. This determines the record count to be processed by a thread. If a file has 10K records, a record count of 2500 with a task count of 4 will mean, each thread will process 2500 records.</li>
<li>In the <strong>Import task count</strong> field, enter the count of import tasks. This must not exceed the max batch threads allocated for batch processing in <strong>System administration &gt;Server configuration</strong>.</li>
</ul>
</li>
</ol>
<h2 id="clean-up-the-staging-tables">Clean up the staging tables</h2>
<p>Starting in Platform update 29, this functionality has been deprecated. This is replaced by a new version of job history clean-up functionality explained below.</p>
<p>You can clean up staging tables by using the <strong>Staging clean up</strong> feature in the <strong>Data management</strong> workspace. You can use the following options to select which records should be deleted from which staging table:</p>
<ul>
<li><strong>Entity</strong> – If only an entity is provided, all records from that entity’s staging table are deleted. Select this option to clean up all the data for the entity across all data projects and all jobs.</li>
<li><strong>Job ID</strong> – If only a job ID is provided, all records for all entities in the selected job are deleted from the appropriate staging tables.</li>
<li><strong>Data projects</strong> – If only a data project is selected, all records for all entities and across all jobs for the selected data project are deleted.</li>
</ul>
<p>You can also combine the options to further restrict the record set that is deleted.</p>
<h2 id="job-history-clean-up-available-in-platform-update-29-and-later">Job history clean up (available in Platform update 29 and later)</h2>
<p>The job history clean-up functionality in data management must be used to schedule a periodic cleanup of the execution history. This functionality replaces the previous staging table clean-up functionality, which is now deprecated. The following tables will be cleaned up by the clean-up process.</p>
<ul>
<li><p>All staging tables</p>
</li>
<li><p>DMFSTAGINGVALIDATIONLOG</p>
</li>
<li><p>DMFSTAGINGEXECUTIONERRORS</p>
</li>
<li><p>DMFSTAGINGLOGDETAIL</p>
</li>
<li><p>DMFSTAGINGLOG</p>
</li>
<li><p>DMFDEFINITIONGROUPEXECUTIONHISTORY</p>
</li>
<li><p>DMFEXECUTION</p>
</li>
<li><p>DMFDEFINITIONGROUPEXECUTION</p>
</li>
</ul>
<p>The <strong>Execution history cleanup</strong> feature must be enabled in feature management and then can be accessed from <strong>Data management &gt; Job history cleanup</strong>.</p>
<h3 id="scheduling-parameters">Scheduling parameters</h3>
<p>When scheduling the clean-up process, the following parameters must be specified to define the clean-up criteria.</p>
<ul>
<li><p><strong>Number of days to retain history</strong> – This setting is used to control the amount of execution history to be preserved. This is specified in number of days. When the clean-up job is scheduled as a recurring batch job, this setting will act like a continuously
moving window thereby, always leaving the history for the specified number of days intact while deleting the rest. The default is 7 days.</p>
</li>
<li><p><strong>Number of hours to execute the job</strong> – Depending on the amount of history to be cleaned up, the total execution time for the clean-up job can vary from a few minutes to a few hours. This parameter must be set to the number of hours that the job will execute. After the clean-up job has executed for the specified number of hours, the job will exit and will resume the clean up the next time it is run based on the recurrence schedule.</p>
<p>A maximum execution time can be specified by setting a max limit on the number of hours the job must run using this setting. The clean-up logic goes through one job execution ID at a time in a chronologically arranged sequence, with oldest being first for the cleanup of related execution history. It will stop picking up new execution ID’s for cleanup when the remaining execution duration is within the last 10% of the specified duration. In some cases, it will be expected that the clean-up job will continue beyond the specified max time. This will largely depend on the number of records to be deleted for the current execution ID that was started before the 10% threshold was reached. The cleanup that was started must be completed to ensure data integrity, which means that cleanup will continue despite exceeding the specified limit. When this is complete, new execution ID’s are not picked up and the clean-up job completes. The remaining execution history that was not cleaned up due to lack of enough execution time, will be picked up the next time the clean-up job is scheduled. The default and minimum value for this setting is set to 2 hours.</p>
</li>
<li><p><strong>Recurring batch</strong> – The clean-up job can be run as a one-time, manual execution, or it can be also scheduled for recurring execution in batch. The batch can be scheduled using the <strong>Run in background</strong> settings, which is the standard batch set up.</p>
</li>
</ul>
<div class="NOTE">
<h5>Note</h5>
<p>If records in the staging tables are not cleaned up completely, ensure that the cleanup job is scheduled to run in recurrence. As explained above, in any clean up execution the job will only clean up as many execution ID's as is possible within the provided maximum hours. In order to continue cleanup of any remaining staging records, the job must be scheduled to run periodically.</p>
</div>
<h2 id="job-history-clean-up-and-archival-available-for-preview-in-platform-update-39-or-version-10015">Job history clean up and archival (available for preview in Platform update 39 or version 10.0.15)</h2>
<p>The job history clean up and archival functionality replaces the previous versions of the clean up functionality. This section will explain these new capabilities.</p>
<p>One of the main changes to the clean up functionality is the use of system batch job for cleaning up the history. The use of system batch job allows Finance and Operations apps to have the clean up batch job automatically scheduled and running as soon as the system is ready. It is no longer required to schedule the batch job manually. In this default execution mode, the batch job will execute every hour starting at 12 midnight and will retain the execution history for the most recent 7 days. The purged history is archived for future retrieval.</p>
<div class="NOTE">
<h5>Note</h5>
<p>Because this functionality is in preview, the system batch job will not delete any execution history until it is enabled via the flight DMFEnableExecutionHistoryCleanupSystemJob. When the feature is generally available in a future release, this flight will not be required and the system batch job will start to purge and archive after the system is ready, based on the defined schedule as explained above.</p>
</div>
<div class="NOTE">
<h5>Note</h5>
<p>In a future release, the previous versions of the clean up functionality will be removed from Finance and Operations apps.</p>
</div>
<p>The second change in the clean up process is the archival of the purged execution history. The clean up job will archive the deleted records to the blob storage that DIXF uses for regular integrations. The archived file will be in the DIXF package format and will be available for 7 days in the blob during which time it can be downloaded. The default longevity of 7 days for the archived file can be changed to a maximum of 90 days in the parameters.</p>
<h3 id="changing-the-default-settings">Changing the default settings</h3>
<p>This functionality is currently in preview and must be explicitly turned on by enabling the flight DMFEnableExecutionHistoryCleanupSystemJob. The staging clean up feature must also be turned on in feature management.</p>
<p>To change the default setting for the longevity of the archived file, go to the data management workspace and select <strong>Job history cleanup</strong>. Set <strong>Days to retain package in blob</strong> to a value between 7 and 90 (inclusive). This will take effect on the archives that are created after this change was made.</p>
<h3 id="downloading-the-archived-package">Downloading the archived package</h3>
<p>This functionality is currently in preview and must be explicitly turned on by enabling the flight DMFEnableExecutionHistoryCleanupSystemJob. The staging clean up feature must also be turned on in feature management.</p>
<p>To download the archived execution history, go to the data management workspace and select <strong>Job history cleanup</strong>. Select <strong>Package backup history</strong> to open the history form. This form shows the list of all archived packages. An archive can be selected and downloaded by selecting <strong>Download package</strong>. The downloaded package will be in the DIXF package format and contain the following files:</p>
<ul>
<li>The entity staging table file</li>
<li>DMFDEFINITIONGROUPEXECUTION</li>
<li>DMFDEFINITIONGROUPEXECUTIONHISTORY</li>
<li>DMFEXECUTION</li>
<li>DMFSTAGINGEXECUTIONERRORS</li>
<li>DMFSTAGINGLOG</li>
<li>DMFSTAGINGLOGDETAILS</li>
<li>DMFSTAGINGVALIDATIONLOG</li>
</ul>
</article>
          </div>
          
          <div class="hidden-sm col-md-2" role="complementary">
            <div class="sideaffix">
              <div class="contribution">
                <ul class="nav">
                </ul>
              </div>
              <nav class="bs-docs-sidebar hidden-print hidden-xs hidden-sm affix" id="affix">
                <h5>In This Article</h5>
                <div></div>
              </nav>
            </div>
          </div>
        </div>
      </div>
      
      <footer>
        <div class="grad-bottom"></div>
        <div class="footer">
          <div class="container">
            <span class="pull-right">
              <a href="#top">Back to top</a>
            </span>
            
            <span>Generated by <strong>DocFX</strong></span>
          </div>
        </div>
      </footer>
    </div>
    
    <script type="text/javascript" src="../../../../styles/docfx.vendor.js"></script>
    <script type="text/javascript" src="../../../../styles/docfx.js"></script>
    <script type="text/javascript" src="../../../../styles/main.js"></script>
  </body>
</html>
